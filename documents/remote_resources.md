# Remote Resources

## Core Dependencies
- [MCP Protocol](https://github.com/modelcontextprotocol/docs)
- [MCP-Go](https://github.com/mark3labs/mcp-go)
- [LangChainGo](https://github.com/tmc/langchaingo)

## LLM Providers
- [OpenAI API](https://platform.openai.com/docs/api-reference)
- [Anthropic API](https://docs.anthropic.com/claude/reference/getting-started-with-the-api)

## Libraries
- [Logrus](https://github.com/sirupsen/logrus)
- [Tiktoken-go](https://github.com/pkoukk/tiktoken-go)

## Dev Tools
- [MCP Inspector](https://www.npmjs.com/package/@modelcontextprotocol/inspector)

## External MCP Tools
- [MCP Filesystem Server](https://github.com/mark3labs/mcp-filesystem-server)
- [MCP Playwright](https://github.com/executeautomation/mcp-playwright)

## Docker
- [speelka-agent Docker Image](https://github.com/korchasa/speelka-agent-go/pkgs/container/speelka-agent)

## LLM Token Counting
- Refer to official OpenAI and Anthropic documentation for token counting details.
- Project uses exact token counts from LLM responses by default, with fallback to a centralized estimation utility if needed.

// All links verified and updated as of 2025-04-15.